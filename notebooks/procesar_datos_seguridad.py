"""
Script para procesar datos de seguridad en M√©xico

Este script transforma los datos raw descargados a formato tidy,
realiza validaciones de calidad y guarda los resultados en data/processed/

Requisitos:
- Datos raw previamente descargados en data/raw/
- Librer√≠as: pandas, numpy

Uso:
    python procesar_datos_seguridad.py
"""

import sys
from pathlib import Path
from datetime import datetime

import pandas as pd
import numpy as np

# Configurar rutas del proyecto
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
DATA_RAW_DIR = PROJECT_ROOT / "data" / "raw"
DATA_PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"
DATA_INTERIM_DIR = PROJECT_ROOT / "data" / "interim"

# Crear directorios si no existen
DATA_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
DATA_INTERIM_DIR.mkdir(parents=True, exist_ok=True)


class ValidadorCalidadDatos:
    """Clase para validar la calidad de los datos"""
    
    def __init__(self):
        self.errores = []
        self.advertencias = []
    
    def validar_valores_nulos(self, df, columnas_criticas, nombre_dataset):
        """Valida que no haya valores nulos en columnas cr√≠ticas"""
        for col in columnas_criticas:
            if col in df.columns:
                nulos = df[col].isnull().sum()
                pct_nulos = (nulos / len(df)) * 100
                
                if nulos > 0:
                    if pct_nulos > 10:
                        self.errores.append(
                            f"{nombre_dataset}: Columna '{col}' tiene {nulos} nulos ({pct_nulos:.2f}%)"
                        )
                    else:
                        self.advertencias.append(
                            f"{nombre_dataset}: Columna '{col}' tiene {nulos} nulos ({pct_nulos:.2f}%)"
                        )
    
    def validar_rango_valores(self, df, columna, min_val, max_val, nombre_dataset):
        """Valida que los valores est√©n en un rango esperado"""
        if columna in df.columns:
            fuera_rango = df[
                (df[columna] < min_val) | (df[columna] > max_val)
            ]
            
            if len(fuera_rango) > 0:
                self.advertencias.append(
                    f"{nombre_dataset}: {len(fuera_rango)} valores en '{columna}' fuera del rango [{min_val}, {max_val}]"
                )
    
    def validar_tipos_datos(self, df, columnas_tipos, nombre_dataset):
        """Valida que las columnas tengan los tipos de datos correctos"""
        for col, tipo_esperado in columnas_tipos.items():
            if col in df.columns:
                tipo_actual = df[col].dtype
                
                if tipo_esperado == 'numeric' and not pd.api.types.is_numeric_dtype(tipo_actual):
                    self.errores.append(
                        f"{nombre_dataset}: Columna '{col}' deber√≠a ser num√©rica pero es {tipo_actual}"
                    )
                elif tipo_esperado == 'string' and not pd.api.types.is_string_dtype(tipo_actual) and tipo_actual != 'object':
                    self.advertencias.append(
                        f"{nombre_dataset}: Columna '{col}' deber√≠a ser string pero es {tipo_actual}"
                    )
    
    def validar_duplicados(self, df, columnas_clave, nombre_dataset):
        """Valida que no haya registros duplicados"""
        duplicados = df.duplicated(subset=columnas_clave, keep=False)
        num_duplicados = duplicados.sum()
        
        if num_duplicados > 0:
            self.advertencias.append(
                f"{nombre_dataset}: {num_duplicados} registros duplicados encontrados"
            )
    
    def validar_completitud_temporal(self, df, col_a√±o, a√±os_esperados, col_grupo, nombre_dataset):
        """Valida que haya datos para todos los a√±os esperados por grupo"""
        if col_grupo in df.columns and col_a√±o in df.columns:
            grupos_incompletos = []
            
            for grupo in df[col_grupo].unique():
                a√±os_disponibles = set(df[df[col_grupo] == grupo][col_a√±o].unique())
                a√±os_faltantes = set(a√±os_esperados) - a√±os_disponibles
                
                if a√±os_faltantes:
                    grupos_incompletos.append(f"{grupo} (faltan a√±os: {sorted(a√±os_faltantes)})")
            
            if grupos_incompletos and len(grupos_incompletos) <= 5:
                self.advertencias.append(
                    f"{nombre_dataset}: Grupos con datos incompletos: {', '.join(grupos_incompletos[:5])}"
                )
            elif grupos_incompletos:
                self.advertencias.append(
                    f"{nombre_dataset}: {len(grupos_incompletos)} grupos con datos incompletos"
                )
    
    def mostrar_resultados(self):
        """Muestra los resultados de las validaciones"""
        print("\n" + "=" * 80)
        print("RESULTADOS DE VALIDACI√ìN DE CALIDAD")
        print("=" * 80)
        
        if self.errores:
            print(f"\n‚ùå ERRORES CR√çTICOS ({len(self.errores)}):")
            for error in self.errores:
                print(f"  ‚Ä¢ {error}")
        
        if self.advertencias:
            print(f"\n‚ö†Ô∏è  ADVERTENCIAS ({len(self.advertencias)}):")
            for advertencia in self.advertencias:
                print(f"  ‚Ä¢ {advertencia}")
        
        if not self.errores and not self.advertencias:
            print("\n‚úÖ Todas las validaciones pasaron correctamente")
        
        print("\n" + "=" * 80)
        
        return len(self.errores) == 0


def procesar_percepcion_inseguridad(validador):
    """
    Procesa datos de percepci√≥n de inseguridad
    
    Args:
        validador: Instancia de ValidadorCalidadDatos
        
    Returns:
        DataFrame procesado
    """
    print("\nüìä Procesando datos de percepci√≥n de inseguridad...")
    
    # Cargar datos raw
    input_file = DATA_RAW_DIR / "indicador_inseguridad_estados.csv"
    if not input_file.exists():
        print(f"‚ùå Error: No se encontr√≥ {input_file}")
        return None
    
    df = pd.read_csv(input_file)
    print(f"  ‚úì Cargados {len(df)} registros")
    
    # 1. Limpieza b√°sica
    df_clean = df.copy()
    
    # Renombrar columnas para consistencia
    df_clean = df_clean.rename(columns={
        'clave': 'cve_entidad'
    })
    
    # 2. Convertir tipos de datos
    df_clean['a√±o'] = df_clean['a√±o'].astype(int)
    df_clean['valor'] = pd.to_numeric(df_clean['valor'], errors='coerce')
    df_clean['cve_entidad'] = df_clean['cve_entidad'].astype(str).str.zfill(2)
    
    # 3. Agregar columnas calculadas
    # Categorizar nivel de percepci√≥n
    df_clean['nivel_percepcion'] = pd.cut(
        df_clean['valor'],
        bins=[0, 50000, 70000, 85000, 100000],
        labels=['Bajo', 'Medio', 'Alto', 'Muy Alto'],
        include_lowest=True
    )
    
    # Indicador de entidad nacional
    df_clean['es_nacional'] = df_clean['entidad'] == 'Nacional'
    
    # 4. Ordenar datos
    df_clean = df_clean.sort_values(['cve_entidad', 'a√±o']).reset_index(drop=True)
    
    # 5. Validaciones de calidad
    print("  ‚Üí Ejecutando validaciones de calidad...")
    
    validador.validar_valores_nulos(
        df_clean,
        columnas_criticas=['a√±o', 'valor', 'entidad', 'cve_entidad'],
        nombre_dataset='Percepci√≥n de Inseguridad'
    )
    
    validador.validar_tipos_datos(
        df_clean,
        columnas_tipos={
            'a√±o': 'numeric',
            'valor': 'numeric',
            'entidad': 'string',
            'cve_entidad': 'string'
        },
        nombre_dataset='Percepci√≥n de Inseguridad'
    )
    
    validador.validar_rango_valores(
        df_clean,
        columna='valor',
        min_val=0,
        max_val=100000,
        nombre_dataset='Percepci√≥n de Inseguridad'
    )
    
    validador.validar_duplicados(
        df_clean,
        columnas_clave=['a√±o', 'cve_entidad'],
        nombre_dataset='Percepci√≥n de Inseguridad'
    )
    
    a√±os_esperados = range(2011, 2026)  # 2011-2025
    validador.validar_completitud_temporal(
        df_clean,
        col_a√±o='a√±o',
        a√±os_esperados=a√±os_esperados,
        col_grupo='entidad',
        nombre_dataset='Percepci√≥n de Inseguridad'
    )
    
    # 6. Guardar datos procesados
    output_file = DATA_PROCESSED_DIR / "percepcion_inseguridad_procesado.csv"
    df_clean.to_csv(output_file, index=False)
    print(f"  ‚úì Guardado: {output_file}")
    
    # Guardar tambi√©n versi√≥n sin nacional (solo estados)
    df_estados = df_clean[df_clean['entidad'] != 'Nacional'].copy()
    output_file_estados = DATA_PROCESSED_DIR / "percepcion_inseguridad_estados.csv"
    df_estados.to_csv(output_file_estados, index=False)
    print(f"  ‚úì Guardado: {output_file_estados}")
    
    return df_clean


def procesar_incidencia_delictiva(validador):
    """
    Procesa datos de incidencia delictiva
    
    Args:
        validador: Instancia de ValidadorCalidadDatos
        
    Returns:
        DataFrame procesado
    """
    print("\nüìä Procesando datos de incidencia delictiva...")
    
    # Cargar datos raw
    input_file = DATA_RAW_DIR / "incidencia_delictiva_estatal_2015_2025.csv"
    if not input_file.exists():
        print(f"‚ùå Error: No se encontr√≥ {input_file}")
        return None
    
    df = pd.read_csv(input_file, encoding='latin-1')
    print(f"  ‚úì Cargados {len(df)} registros")
    
    # Mostrar columnas para entender la estructura
    print(f"  ‚Üí Columnas detectadas: {df.columns.tolist()[:5]}...")
    
    # Normalizar nombres de columnas (quitar espacios, min√∫sculas)
    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')
    
    # Detectar columnas de a√±o y mes (pueden variar)
    col_a√±o = None
    col_mes = None
    
    for col in df.columns:
        if 'a√±o' in col or 'ano' in col or col == 'a√±o':
            col_a√±o = col
        elif 'mes' in col:
            col_mes = col
    
    if not col_a√±o:
        # Buscar columna num√©rica que parezca a√±o
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64']:
                valores_unicos = df[col].unique()
                if any(2015 <= v <= 2025 for v in valores_unicos if pd.notna(v)):
                    col_a√±o = col
                    break
    
    print(f"  ‚Üí Columna de a√±o detectada: {col_a√±o}")
    print(f"  ‚Üí Columna de mes detectada: {col_mes}")
    
    # Guardar versi√≥n intermedia con toda la informaci√≥n
    interim_file = DATA_INTERIM_DIR / "incidencia_delictiva_completa.csv"
    df.to_csv(interim_file, index=False)
    print(f"  ‚úì Guardado archivo intermedio: {interim_file}")
    
    # Crear versi√≥n agregada simple (si es posible)
    if col_a√±o:
        # Intentar crear agregaci√≥n por a√±o y entidad
        cols_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # Remover columnas que no son conteos
        cols_excluir = [col_a√±o, col_mes] if col_mes else [col_a√±o]
        cols_conteo = [c for c in cols_numericas if c not in cols_excluir]
        
        if cols_conteo:
            print(f"  ‚Üí Creando agregaci√≥n anual...")
            
            # Agrupar por a√±o (y entidad si existe)
            cols_grupo = [col_a√±o]
            col_entidad = None
            
            for col in df.columns:
                if 'entidad' in col or 'estado' in col:
                    col_entidad = col
                    cols_grupo.append(col)
                    break
            
            # Crear DataFrame simple procesado
            df_procesado = df[cols_grupo + cols_conteo].copy()
            
            # Validaciones b√°sicas
            if col_a√±o:
                validador.validar_valores_nulos(
                    df_procesado,
                    columnas_criticas=cols_grupo,
                    nombre_dataset='Incidencia Delictiva'
                )
            
            output_file = DATA_PROCESSED_DIR / "incidencia_delictiva_procesado.csv"
            df_procesado.to_csv(output_file, index=False)
            print(f"  ‚úì Guardado: {output_file}")
            
            return df_procesado
    
    print("  ‚ö†Ô∏è  Estructura de datos compleja - solo se guard√≥ versi√≥n intermedia")
    return df


def generar_reporte_procesamiento(df_percepcion, df_delictiva):
    """
    Genera un reporte del procesamiento realizado
    
    Args:
        df_percepcion: DataFrame procesado de percepci√≥n
        df_delictiva: DataFrame procesado de delictiva
    """
    reporte_file = DATA_PROCESSED_DIR / "reporte_procesamiento.txt"
    
    with open(reporte_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("REPORTE DE PROCESAMIENTO DE DATOS DE SEGURIDAD\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"Fecha de procesamiento: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # Percepci√≥n de Inseguridad
        if df_percepcion is not None:
            f.write("\n" + "=" * 80 + "\n")
            f.write("DATASET 1: Percepci√≥n de Inseguridad\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"Total de registros: {len(df_percepcion)}\n")
            f.write(f"Per√≠odo: {df_percepcion['a√±o'].min()} - {df_percepcion['a√±o'].max()}\n")
            f.write(f"Entidades: {df_percepcion['entidad'].nunique()}\n")
            f.write(f"\nColumnas generadas:\n")
            for col in df_percepcion.columns:
                f.write(f"  - {col}: {df_percepcion[col].dtype}\n")
            
            f.write(f"\nEstad√≠sticas del indicador 'valor':\n")
            f.write(f"  - Media: {df_percepcion['valor'].mean():.2f}\n")
            f.write(f"  - Mediana: {df_percepcion['valor'].median():.2f}\n")
            f.write(f"  - Desv. Std: {df_percepcion['valor'].std():.2f}\n")
            f.write(f"  - M√≠nimo: {df_percepcion['valor'].min():.2f}\n")
            f.write(f"  - M√°ximo: {df_percepcion['valor'].max():.2f}\n")
            
            f.write(f"\nDistribuci√≥n por nivel de percepci√≥n:\n")
            if 'nivel_percepcion' in df_percepcion.columns:
                for nivel, count in df_percepcion['nivel_percepcion'].value_counts().items():
                    pct = count / len(df_percepcion) * 100
                    f.write(f"  - {nivel}: {count} ({pct:.1f}%)\n")
        
        # Incidencia Delictiva
        if df_delictiva is not None:
            f.write("\n" + "=" * 80 + "\n")
            f.write("DATASET 2: Incidencia Delictiva\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"Total de registros: {len(df_delictiva)}\n")
            f.write(f"Columnas: {len(df_delictiva.columns)}\n")
            f.write(f"\nPrimeras columnas:\n")
            for col in df_delictiva.columns[:10]:
                f.write(f"  - {col}: {df_delictiva[col].dtype}\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("FIN DEL REPORTE\n")
        f.write("=" * 80 + "\n")
    
    print(f"\n‚úì Reporte de procesamiento guardado: {reporte_file}")


def main():
    """Funci√≥n principal del script"""
    print("=" * 80)
    print("PROCESAMIENTO DE DATOS DE SEGURIDAD - M√âXICO")
    print("=" * 80)
    print(f"\nDirectorio de datos raw: {DATA_RAW_DIR}")
    print(f"Directorio de datos procesados: {DATA_PROCESSED_DIR}")
    print(f"Directorio de datos intermedios: {DATA_INTERIM_DIR}")
    print(f"\nFecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Verificar que existan los datos raw
    archivos_requeridos = [
        "indicador_inseguridad_estados.csv",
        "incidencia_delictiva_estatal_2015_2025.csv"
    ]
    
    archivos_faltantes = []
    for archivo in archivos_requeridos:
        if not (DATA_RAW_DIR / archivo).exists():
            archivos_faltantes.append(archivo)
    
    if archivos_faltantes:
        print(" ERROR: Faltan archivos raw necesarios:")
        for archivo in archivos_faltantes:
            print(f"  ‚Ä¢ {archivo}")
        print("\nPor favor, ejecuta primero el script de descarga:")
        print("  python datos_seguridad_mexico.py --token TU_TOKEN")
        sys.exit(1)
    
    # Crear validador de calidad
    validador = ValidadorCalidadDatos()
    
    # Procesar datasets
    df_percepcion = procesar_percepcion_inseguridad(validador)
    df_delictiva = procesar_incidencia_delictiva(validador)
    
    # Mostrar resultados de validaci√≥n
    validacion_exitosa = validador.mostrar_resultados()
    
    # Generar reporte
    generar_reporte_procesamiento(df_percepcion, df_delictiva)
    
    print("\n" + "=" * 80)
    print("¬°PROCESAMIENTO COMPLETADO!")
    print("=" * 80)
    print(f"\nArchivos generados:")
    print(f"\nProcesados (data/processed/):")
    print("  ‚Ä¢ percepcion_inseguridad_procesado.csv")
    print("  ‚Ä¢ percepcion_inseguridad_estados.csv")
    print("  ‚Ä¢ incidencia_delictiva_procesado.csv")
    print("  ‚Ä¢ reporte_procesamiento.txt")
    print(f"\nIntermedios (data/interim/):")
    print("  ‚Ä¢ incidencia_delictiva_completa.csv")
    
    if not validacion_exitosa:
        print("\n‚ö†Ô∏è  ATENCI√ìN: Se encontraron errores de calidad. Revisa el reporte arriba.")
        sys.exit(1)


if __name__ == "__main__":
    main()
